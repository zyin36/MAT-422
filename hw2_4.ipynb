{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4MUDvZx3ZULFLfb6wyhta",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zyin36/MAT-422/blob/main/hw2_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Maximum Likelihood Estimation**\n",
        "\n",
        "### **MLE for Random Samples**\n",
        "Goal: Estimate the parameters of a probability distribution through maximizing a likelihood function\n",
        "\n",
        "**Definition 2.4.1**:\n",
        "Let $X_1, X_2, ..., X_n$ be a random sample, which has a joint pdf $f(x_1, x_2, ..., x_n; ﾎｸ_1,...,ﾎｸ_m)$, where $x_1,...,x_n$ are observed sample values and $ﾎｸ_1,...,ﾎｸ_m$ are parameters. When we regard this joint pdf as a function of $ﾎｸ_1,...,ﾎｸ_m$ and $x_1,...,x_n$ is constant, it is the likelihood function. We find MLE $\\hat{ﾎｸ_1}, ..., \\hat{ﾎｸ_m}$ such that the likelihood function is maximized, i.e.,  $f(x_1, ..., x_n; \\hat{ﾎｸ_1}, ..., \\hat{ﾎｸ_m}) 竕･ f(x_1, ..., x_n; ﾎｸ_1,...,ﾎｸ_m)$ for all $ﾎｸ_1, ..., ﾎｸ_m$.\n",
        "\n",
        "  Example (from 2.3): Let $X_1, X_2, ..., X_n$ be a random sample from a normal distribution.\n",
        "\n",
        "  \\begin{align}\n",
        "        f(x_1, x_2, ..., x_n; ﾎｼ, ﾏタ2) = \\left(\\frac{1}{2ﾏﾏタ2}\\right)^\\frac{n}{2}  e^{\\frac{-ﾎ｣(x_i - ﾎｼ)^2}{2ﾏタ2}}\n",
        "  \\end{align}\n",
        "is the likelihood function.\n",
        "\n",
        "Finding the partial derivatives of $ln(f)$ wrt $ﾎｼ$ and $ﾏタ2$ respectively, then equating them to zero and solve for $ﾎｼ$ and $ﾏタ2$ respectively gives us the maximizing values -\n",
        "  \\begin{align}\n",
        "  \\hat{ﾎｼ} = \\bar{X} \\\\[1em] \\hat{ﾏマ^2 = \\frac{ﾎ｣(X_i - \\bar{X})^2}{n}\n",
        "  \\end{align}"
      ],
      "metadata": {
        "id": "Ga0QUJGFo01i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Linear Regression**\n",
        "In a dataset, we are usually interested in the correlation between a set for independent variables ($攝ｱ$) and a dependent variable ($y$).\n",
        "\n",
        "Goal: Given data points $\\{(攝ｱ_i, y_i)\\}_{i=1}^n$, where each $攝ｱ_i = (x_{i1},...,x_{ip})$, and the estimation of $y_i$ is\n",
        "  \\begin{align}\n",
        "  \\hat{y_i} = ﾎｲ_0 + ﾎｲ_1x_{i1} + ... + ﾎｲ_px_{ip}\n",
        "  \\end{align}\n",
        "Find coeffients $ﾎｲ_1,...,ﾎｲ_p$ such that the sum of residual squared is minimized.\n",
        "\n",
        "$y_i = \\hat{y_i} + 撩$, where $ﾎｵ 竏ｼ N(0, ﾏタ2)$, so $y_i 竏ｼ N(\\hat{y_i}, ﾏタ2)$.\n",
        "\n",
        "Based on the derivation from notes, we find that MLE of $ﾎｲ$ is exactly the least square problem.\n",
        "\n"
      ],
      "metadata": {
        "id": "lvqFaNHnR6wX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "# load diabetes dataset\n",
        "df, target = datasets.load_diabetes(return_X_y=True, as_frame=True)\n",
        "# Drop the sex column\n",
        "df = df.drop('sex', axis=1)\n",
        "df = df.astype({\"age\": int, \"bp\": int})\n",
        "\n",
        "print(\"df shape: \", df.shape)\n",
        "print(\"target shape: \", target.shape)\n",
        "\n",
        "print(\"df datatypes: \\n\", df.dtypes)\n",
        "print(\"target datatype: \\n\", target.dtypes)\n",
        "\n",
        "# linear regression\n",
        "A = df.to_numpy()\n",
        "b = target.to_numpy()\n",
        "\n",
        "print(\"A shape: \", A.shape)\n",
        "print(\"b shape: \", b.shape)\n",
        "\n",
        "x_hat = np.linalg.pinv(A.T @ A) @ A.T @ b\n",
        "print(\"x_hat: \", x_hat)"
      ],
      "metadata": {
        "id": "dmKHY85viDdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c7aec7-e397-4029-b142-7fd30f60cf34"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df shape:  (442, 9)\n",
            "target shape:  (442,)\n",
            "df datatypes: \n",
            " age      int64\n",
            "bmi    float64\n",
            "bp       int64\n",
            "s1     float64\n",
            "s2     float64\n",
            "s3     float64\n",
            "s4     float64\n",
            "s5     float64\n",
            "s6     float64\n",
            "dtype: object\n",
            "target datatype: \n",
            " float64\n",
            "A shape:  (442, 9)\n",
            "b shape:  (442,)\n",
            "x_hat:  [ 0.00000000e+00  6.20111375e+02  1.72109029e-11 -6.64492941e+02\n",
            "  4.03282990e+02  1.02576367e+02  7.14520175e+01  8.02623275e+02\n",
            "  1.02750426e+02]\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4MUDvZx3ZULFLfb6wyhta",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zyin36/MAT-422/blob/main/hw2_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Maximum Likelihood Estimation**\n",
        "\n",
        "### **MLE for Random Samples**\n",
        "Goal: Estimate the parameters of a probability distribution through maximizing a likelihood function\n",
        "\n",
        "**Definition 2.4.1**:\n",
        "Let $X_1, X_2, ..., X_n$ be a random sample, which has a joint pdf $f(x_1, x_2, ..., x_n; θ_1,...,θ_m)$, where $x_1,...,x_n$ are observed sample values and $θ_1,...,θ_m$ are parameters. When we regard this joint pdf as a function of $θ_1,...,θ_m$ and $x_1,...,x_n$ is constant, it is the likelihood function. We find MLE $\\hat{θ_1}, ..., \\hat{θ_m}$ such that the likelihood function is maximized, i.e.,  $f(x_1, ..., x_n; \\hat{θ_1}, ..., \\hat{θ_m}) ≥ f(x_1, ..., x_n; θ_1,...,θ_m)$ for all $θ_1, ..., θ_m$.\n",
        "\n",
        "  Example (from 2.3): Let $X_1, X_2, ..., X_n$ be a random sample from a normal distribution.\n",
        "\n",
        "  \\begin{align}\n",
        "        f(x_1, x_2, ..., x_n; μ, σ^2) = \\left(\\frac{1}{2πσ^2}\\right)^\\frac{n}{2}  e^{\\frac{-Σ(x_i - μ)^2}{2σ^2}}\n",
        "  \\end{align}\n",
        "is the likelihood function.\n",
        "\n",
        "Finding the partial derivatives of $ln(f)$ wrt $μ$ and $σ^2$ respectively, then equating them to zero and solve for $μ$ and $σ^2$ respectively gives us the maximizing values -\n",
        "  \\begin{align}\n",
        "  \\hat{μ} = \\bar{X} \\\\[1em] \\hat{σ}^2 = \\frac{Σ(X_i - \\bar{X})^2}{n}\n",
        "  \\end{align}"
      ],
      "metadata": {
        "id": "Ga0QUJGFo01i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Linear Regression**\n",
        "In a dataset, we are usually interested in the correlation between a set for independent variables ($𝐱$) and a dependent variable ($y$).\n",
        "\n",
        "Goal: Given data points $\\{(𝐱_i, y_i)\\}_{i=1}^n$, where each $𝐱_i = (x_{i1},...,x_{ip})$, and the estimation of $y_i$ is\n",
        "  \\begin{align}\n",
        "  \\hat{y_i} = β_0 + β_1x_{i1} + ... + β_px_{ip}\n",
        "  \\end{align}\n",
        "Find coeffients $β_1,...,β_p$ such that the sum of residual squared is minimized.\n",
        "\n",
        "$y_i = \\hat{y_i} + 𝜀$, where $ε ∼ N(0, σ^2)$, so $y_i ∼ N(\\hat{y_i}, σ^2)$.\n",
        "\n",
        "Based on the derivation from notes, we find that MLE of $β$ is exactly the least square problem.\n",
        "\n"
      ],
      "metadata": {
        "id": "lvqFaNHnR6wX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "# load diabetes dataset\n",
        "df, target = datasets.load_diabetes(return_X_y=True, as_frame=True)\n",
        "# Drop the sex column\n",
        "df = df.drop('sex', axis=1)\n",
        "df = df.astype({\"age\": int, \"bp\": int})\n",
        "\n",
        "print(\"df shape: \", df.shape)\n",
        "print(\"target shape: \", target.shape)\n",
        "\n",
        "print(\"df datatypes: \\n\", df.dtypes)\n",
        "print(\"target datatype: \\n\", target.dtypes)\n",
        "\n",
        "# linear regression\n",
        "A = df.to_numpy()\n",
        "b = target.to_numpy()\n",
        "\n",
        "print(\"A shape: \", A.shape)\n",
        "print(\"b shape: \", b.shape)\n",
        "\n",
        "x_hat = np.linalg.pinv(A.T @ A) @ A.T @ b\n",
        "print(\"x_hat: \", x_hat)"
      ],
      "metadata": {
        "id": "dmKHY85viDdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c7aec7-e397-4029-b142-7fd30f60cf34"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df shape:  (442, 9)\n",
            "target shape:  (442,)\n",
            "df datatypes: \n",
            " age      int64\n",
            "bmi    float64\n",
            "bp       int64\n",
            "s1     float64\n",
            "s2     float64\n",
            "s3     float64\n",
            "s4     float64\n",
            "s5     float64\n",
            "s6     float64\n",
            "dtype: object\n",
            "target datatype: \n",
            " float64\n",
            "A shape:  (442, 9)\n",
            "b shape:  (442,)\n",
            "x_hat:  [ 0.00000000e+00  6.20111375e+02  1.72109029e-11 -6.64492941e+02\n",
            "  4.03282990e+02  1.02576367e+02  7.14520175e+01  8.02623275e+02\n",
            "  1.02750426e+02]\n"
          ]
        }
      ]
    }
  ]
}